# Train T5 language model

## Datasets

### FL-Dataset

Sevral flavors of the 2019 [fl-dataset](https://jetbrains.team/p/ccrm/repositories/fl-dataset/files/docs/README.md).

#### Python: Top 5k repos >50 stars
```
wget 'https://5k-dataset.s3.amazonaws.com/v3/dataset-normalized-5000-with-imports.tar.gz'
```
 * Total 482Mb compressed, 3Gb uncompressed
 * Python 2.2Gb
 * Projects 3047
 * Files 270,058
 * Lines 46,431,253
 * (Sub-)Tokens 392,272,175

After filtering: 210k uniq files / 1.6Gb
 * size < 1Mb
 * max line < 1000, avg line len < 100
 * auto-generated by gRPC, protobuf, flatbuf, apache thrift, SWIG

Pre-processed
 * SPE vocabulary https://storage.cloud.google.com/cc-eval/py5k-50.model
 * data https://storage.cloud.google.com/cc-eval/py5k-50-uniq.jsonl.xz

#### Python: all repos >50 stars
 * Total 143Gb compressed, 840Gb uncompressed
 * Python 17Gb
 * Projects 37,847
 * Files 1,745,450
 * Lines 347,563,305

 After filtering: 1,052,596 uniq files / 17Gb

```
aria2c -x 16 -j 16 https://5k-dataset.s3.amazonaws.com/v3/dataset-open-50-more-1.tar.gz
```

#### Python: all repos >10 stars
1Tb total uncompressed

#### Java
TBD

#### Preprocessing

These steps are the same for all the Fulline datasets. Below are examples for the [Python: Top 5k repos >50 stars](#)

```
virtualenv -p python3 .venv
source .venv/bin/activat
pip install -r requirements-preprocess.txt
```


1. Convert dataset to JSONL format
```
./convert-fulline-to-jsonl.py --data_dir='data/dataset-normalized-5000-with-imports'

pv data/dataset-normalized-5000-with-imports/*.jsonl \
  > data/jsonl/py5k-50.jsonl

# remove duplicated files by sha
pv data/jsonl/py5k-50.jsonl \
  | go run filter_dup_sha.go \
  > data/jsonl/py5k-50-uniq.jsonl
```

2. Train SentencePiece vocabulary on full content.
Follow https://github.com/google/sentencepiece#build-and-install-sentencepiece-command-line-tools-from-c-source

```
pv data/jsonl/py5k-50-uniq.jsonl \
  | jq -cr '.content' \
  > data/py5k-50-uniq.txt

# there is no need to do it, but in case you want to recover newlines do
head data/py5k-50-uniq-content.txt | sed 's/ÄŠ/\
/g'

# takes ~10min and 31Gb RAM on 96-core machine
time spm_train \
    --allow_whitespace_only_pieces \
    --noremove_extra_whitespaces \
    --pad_id=0 --eos_id=1 --unk_id=2 --bos_id=-1 \
    --num_threads=96 \
    --vocab_size=32000 \
    --model_prefix=py5k-50 \
    --input=data/py5k-50-uniq.txt
```

3. Generate train/validation splits
Split should preferable be done by-project, to avoid leakig information between splits.
By default, files are order by project, so we are "leaking" at most one project.
```
wc -l data/py5k-50-uniq.txt
 210815
head -n 170000 data/py5k-50-uniq.txt > data/py5k-50.train.txt
tail -n 40815 data/py5k-50-uniq.txt > data/py5k-50.test.txt
```

Split by-file may be generated the same way, just shuffle lines of JSONL before
```
pv py5k-50-uniq.jsonl | perl -MList::Util=shuffle -e 'print shuffle <>;'
```

Create 5 shards
```
split -da 4 -l $((`wc -l < data/py5k-50.train.txt`/5)) data/py5k-50.train.txt data/py5k-50.train.txt- --additional-suffix="-of-0005"
```

Change `DATA_DIR` in `tasks.py`.


## Train the model

```
virtualenv -p python3 .venv-train
source .venv-train/bin/activat
pip install -r requirements-train.txt
```

To test LM Task definiton and input pipeline
```
python ./print_dataset.py
```


```
export PROJECT=<project-id>
export ZONE=<zone>
export TPU_NAME=t5-tpu
export TPU_SIZE=v2-8
export BUCKET=gs://t5-codex/
export DATA_DIR="${BUCKET}/data"
export MODEL_DIR="${BUCKET}/models"
export TASK_NAME='py_50stars_top5k_2019'
```

Create TPU from Cloud VM
```
ctpu up --name=$TPU_NAME --project=$PROJECT --zone=$ZONE --tpu-size=$TPU_SIZE \
        --tpu-only --noconf
```

Train
```
python -m t5.models.mesh_transformer_main \
  --tpu="${TPU_NAME}" \
  --gcp_project="${PROJECT}" \
  --tpu_zone="${ZONE}" \
  --model_dir="${MODEL_DIR}" \
  --t5_tfds_data_dir="${DATA_DIR}" \
  --module_import="codeT5.tasks" \
  --gin_location_prefix="codeT5/gin/" \
  --gin_file="models/shared-prefix_lm.gin" \
  --gin_param="utils.tpu_mesh_shape.model_parallelism = 2" \
  --gin_param="utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'" \
  --gin_param="run.keep_checkpoint_max = 8" \
  --gin_param="MIXTURE_NAME = '${TASK_NAME}'" \
  --gin_param="run.train_steps = 10000" \
  --gin_param="mesh_train_dataset_fn.use_cached = True" \
  --additional_task_cache_dirs='${BUCKET}/cache'
```

```

Un-cached .txt \w FunctionDataSource:
 * TPU v2-8, model_parallelism = 2
   3h -> 9k steps
   global_step/sec: 0.89
   examples/sec: 110

 * TPU v2-8, model_parallelism = 1
   global_step/sec: 0.97
   examples/sec: 124
   FLOPS Utilization
    * Utilization of TPU Matrix Units: 61.8%
    * Program's Optimal FLOPS: 56.6%
   Memory Bandwidth Utilization: 40%

Cached .tfrecords \w TextLineDataSource:
 * TPU v2-8, model_parallelism = 2
   global_step/sec: 0.87
   examples/sec: 111
   FLOPS Utilization
     * Utilization of TPU Matrix Units: 56.2%
     * Program's Optimal FLOPS: 51.5%
   Memory Bandwidth Utilization: 43.5%

 * TPU v2-8, model_parallelism = 1
   global_step/sec: 0.97
   examples/sec: 124
   FLOPS Utilization
     * Utilization of TPU Matrix Units: 61.9%
     * Program's Optimal FLOPS: 56.7%
   Memory Bandwidth Utilization: 40.6%

To cache it
```
gcloud auth application-default login
pip install apache-beam[gcp] python-snappy
python -m seqio.scripts.cache_tasks_main \
 --module_import="codeT5.tasks" \
 --tasks="${TASK_NAME}" \
 --output_cache_dir="${BUCKET}/cache" \
 --alsologtostderr \
 --pipeline_options=["--runner=DirectRunner","--direct_num_workers 10"]
```


Eval on latest checkpoint
(27 Min initial padding on un-cached dataset :/)
```
python -m t5.models.mesh_transformer_main  \
  --tpu="$TPU_ADDRESS" \
  --model_dir="$MODEL_DIR" \
  --t5_tfds_data_dir="$DATA_DIR" \
  --module_import="codeT5.tasks" \
  --gin_location_prefix="codeT5/gin/" \
  --gin_file="models/shared-prefix_lm.gin" \
  --gin_file="eval.gin" \
  --gin_file="beam_search.gin" \
  --gin_param="utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'" \
  --gin_param="split = 'validation'" \
  --gin_param="eval_checkpoint_step = -1" \
  --gin_param="MIXTURE_NAME = '${TASK_NAME}'"

  --limit number of steps?
  --use cache?
```