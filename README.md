# Train T5 language model

## Datasets

### Fullline

Flavors of the 2019 dataset used by [Fulline Completion](https://jetbrains.team/p/ccrm/repositories/fl-dataset/files/docs/README.md) project.

#### Python: Top 5k repos >50 stars
```
wget 'https://5k-dataset.s3.amazonaws.com/v3/dataset-normalized-5000-with-imports.tar.gz'
```
 * Total 482Mb compressed, 3Gb uncompressed
 * Python 2.2Gb
 * Projects 3047
 * Files 270,058
 * Lines 46,431,253
 * (Sub-)Tokens 369,241,313

After filtering: 210k uniq files / 1.6Gb
 * size < 1Mb
 * max line < 1000, avg line len < 100
 * auto-generated by gRPC, protobuf, flatbuf, apache thrift, SWIG

Pre-processed
 * SPE vocabulary https://storage.cloud.google.com/cc-eval/py5k-50.model
 * data https://storage.cloud.google.com/cc-eval/py5k-50-uniq.jsonl.xz

#### Python: all repos >50 stars
 * Total 143Gb compressed, 840Gb uncompressed
 * Python 17Gb
 * Projects 35,954
 * Files 1,745,450
 * Lines 347,563,305

```
aria2c -x 16 -j 16 https://5k-dataset.s3.amazonaws.com/v3/dataset-open-50-more-1.tar.gz
```

#### Python: all repos >10 stars
1Tb total uncompressed

#### Java
TBD

#### Preprocessing

These steps are the same for all the Fulline datasets. Below are examples for the [Python: Top 5k repos >50 stars](#)

```
virtualenv -p python3 .venv
source .venv/bin/activat
pip install -r requirements-preprocess.txt
```


1. Convert dataset to JSONL format
```
./convert-fulline-to-jsonl.py --data_dir='data/dataset-normalized-5000-with-imports'

pv data/dataset-normalized-5000-with-imports/*.jsonl \
  > data/jsonl/py5k-50.jsonl

# remove duplicated files by sha
pv data/jsonl/py5k-50.jsonl \
  | go run filter_dup_sha.go \
  > data/jsonl/py5k-50-uniq.jsonl
```

2. Train SentencePiece vocabulary on full content.
Follow https://github.com/google/sentencepiece#build-and-install-sentencepiece-command-line-tools-from-c-source

```
pv data/jsonl/py5k-50-uniq.jsonl \
  | jq -cr '.content' \
  > data/py5k-50-uniq.txt

# there is no need to do it, but in case you want to recover newlines do
head data/py5k-50-uniq-content.txt | sed 's/ÄŠ/\
/g'

# takes ~10min and 31Gb RAM on 96-core machine
time spm_train \
    --allow_whitespace_only_pieces \
    --noremove_extra_whitespaces \
    --pad_id=0 --eos_id=1 --unk_id=2 --bos_id=-1 \
    --num_threads=96 \
    --vocab_size=32000 \
    --model_prefix=py5k-50 \
    --input=data/py5k-50-uniq.txt
```

3. Generate train/validation splits
Split should preferable be done by-project, to avoid leakig information between splits.
By default, files are order by project, so we are "leaking" at most one project.
```
wc -l data/py5k-50-uniq.txt
 210815
head -n 170000 data/py5k-50-uniq.txt > data/py5k-50.train.txt
tail -n +40815 data/py5k-50-uniq.txt > data/py5k-50.test.txt
```

Split by-file may be generated the same way, just shuffle lines of JSONL before
```
pv py5k-50-uniq.jsonl | perl -MList::Util=shuffle -e 'print shuffle <>;'
```



## Train the model

```
virtualenv -p python3 .venv-train
source .venv-train/bin/activat
pip install -r requirements-train.txt
```

To test LM Task definiton and input pipeline
```
python ./print_dataset.py
```


```
export PROJECT=<project-id>
export ZONE=<zone>
export BUCKET=gs://t5-codex/
export TPU_NAME=t5-tpu
export TPU_SIZE=v2-8
export DATA_DIR="${BUCKET}/data"
export MODEL_DIR="${BUCKET}/models"
```

Create TPU from Cloud VM
```
ctpu up --name=$TPU_NAME --project=$PROJECT --zone=$ZONE --tpu-size=$TPU_SIZE \
        --tpu-only --noconf
```

Train
```
t5_mesh_transformer  \
  --tpu="${TPU_NAME}" \
  --gcp_project="${PROJECT}" \
  --tpu_zone="${ZONE}" \
  --model_dir="${MODEL_DIR}" \
  --t5_tfds_data_dir="${DATA_DIR}" \
  --module_import=codeT5
  --gin_file="models/shared-prefix_lm.gin" \
  --gin_param="utils.tpu_mesh_shape.model_parallelism = 1" \
  --gin_param="utils.tpu_mesh_shape.tpu_topology = '${TPU_SIZE}'" \
  --gin_param="MIXTURE_NAME = 'py5k_prefix_lm'"
```