{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSOY2hKYHHDH",
        "outputId": "d2f26be3-6f94-4cd8-fb00-afdd673365e7"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "[ ! -d \"/content/code-t5\" ] && git clone 'https://github.com/bzz/code-t5.git'\n",
        "cd code-t5/\n",
        "git pull origin master --rebase"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXkBeXzCFt35",
        "outputId": "b48474f6-2352-4b22-f363-b1def3e31b27"
      },
      "outputs": [],
      "source": [
        "print(\"Installing dependencies...\")\n",
        "%tensorflow_version 2.x\n",
        "!pip install -qr code-t5/requirements-train.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NknEpagWWVP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = \"gs://t5-codex\" #@param { type: \"string\" }\n",
        "if not BASE_DIR or BASE_DIR == \"gs://\":\n",
        "  raise ValueError(\"You must enter a BASE_DIR.\")\n",
        "DATA_DIR = os.path.join(BASE_DIR, \"data\")\n",
        "MODELS_DIR = os.path.join(BASE_DIR, \"models\")\n",
        "\n",
        "MODEL_SIZE = \"arch-lm_v1-lm\" #@param[\"small\", \"base\", \"base-t5.1.1\", \"base_shared\", \"base_shared_1k\", \"base-top5k\", \"base-top5k\", \"lm_ifa_1k\", \"arch-lm_v1-lm\", \"large\", \"3B\", \"11B\"]\n",
        "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n",
        "CACHE_DIR=\"${BASE_DIR}/cache\"\n",
        "\n",
        "\n",
        "ON_CLOUD = True\n",
        "\n",
        "TRAIN_STEPS = 200000 #@param {type: \"integer\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0APdooM8Jy63",
        "outputId": "a3c1590a-6d78-4e8b-fb40-de7dad1955a6"
      },
      "outputs": [],
      "source": [
        "import functools\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import t5\n",
        "import t5.models\n",
        "import seqio\n",
        "\n",
        "\n",
        "if ON_CLOUD:\n",
        "  print(\"Setting up GCS access...\")\n",
        "  import tensorflow_gcs_config\n",
        "  from google.colab import auth\n",
        "  # Set credentials for GCS reading/writing from Colab and TPU.\n",
        "  TPU_TOPOLOGY = \"v2-8\"\n",
        "  try:\n",
        "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
        "    TPU_ADDRESS = tpu.get_master()\n",
        "    print('Running on TPU:', TPU_ADDRESS)\n",
        "  except ValueError:\n",
        "    raise BaseException('ERROR: Not connected to a TPU runtime')\n",
        "  auth.authenticate_user()\n",
        "  tf.enable_eager_execution()\n",
        "  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n",
        "\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "# Improve logging.\n",
        "from contextlib import contextmanager\n",
        "import logging as py_logging\n",
        "\n",
        "if ON_CLOUD:\n",
        "  tf.get_logger().propagate = False\n",
        "  py_logging.root.setLevel('INFO')\n",
        "\n",
        "@contextmanager\n",
        "def tf_verbosity_level(level):\n",
        "  log_level = tf.logging.get_verbosity()\n",
        "  tf.logging.set_verbosity(level)\n",
        "  yield\n",
        "  tf.logging.set_verbosity(log_level)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gS143v6OHNT0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4xYKKbNIG6d"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ez0ujjN5RyI-"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/code-t5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tyMvPz5jIPU"
      },
      "source": [
        "## Preview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BIfttcBXyQHJ",
        "outputId": "7e5ed6f3-3567-4626-838d-f01490b99b8c"
      },
      "outputs": [],
      "source": [
        "# the list of Tasks we support https://github.com/google/seqio#defining-a-task\n",
        "import codeT5.tasks\n",
        "seqio.TaskRegistry.names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMNSNfToHlYn",
        "outputId": "e45ed577-2a33-4faa-f66f-ee6d74212926"
      },
      "outputs": [],
      "source": [
        "import codeT5.tasks\n",
        "import gin\n",
        "import seqio.utils\n",
        "from t5.data import preprocessors\n",
        "\n",
        "task = seqio.TaskRegistry.get(\"fl_py_50stars_top5k_2019\")\n",
        "seqio.utils.add_global_cache_dirs([os.path.join(BASE_DIR, \"cache\")])\n",
        "\n",
        "vocab = codeT5.tasks.vocab\n",
        "\n",
        "ds = task.get_dataset(split=\"validation\", sequence_length={\"inputs\": 128, \"targets\": 32}, use_cached=True)\n",
        "with gin.unlock_config(): \n",
        "  ## un-comment to configure preprocessing for unsupervised LM pre-training\n",
        "  # gin.bind_parameter(\"preprocessors.unsupervised.preprocessors\", [\n",
        "  #   preprocessors.select_random_chunk,\n",
        "  #   preprocessors.reduce_concat_tokens,\n",
        "  #   preprocessors.split_tokens_to_targets_length,\n",
        "  # ])\n",
        "  gin.bind_parameter(\"preprocessors.select_random_chunk.max_length\", 65536)\n",
        "\n",
        "\n",
        "print(\"A few preprocessed validation examples...\")\n",
        "for ex in tfds.as_numpy(ds.take(5)):\n",
        "  print(ex)\n",
        "  if \"inputs\" in ex:\n",
        "    print(\"inputs: '\" + vocab.decode(ex['inputs'].tolist()).replace(\"Ċ\", \"\\n\") + \"'\")\n",
        "    print()\n",
        "  print(\"targets: '\" + vocab.decode(ex['targets'].tolist()).replace(\"Ċ\", \"\\n\") + \"'\")\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UABp0V_BiNm9"
      },
      "source": [
        "## Token-level stats\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0yHBDUKhUtA"
      },
      "outputs": [],
      "source": [
        "import codeT5\n",
        "\n",
        "# Comment this task in tasks.py first!\n",
        "#\n",
        "# A new task that does not sample, truncate or pack examples\n",
        "# the name has to be the same, to levirage GCS cache\n",
        "seqio.TaskRegistry.add(\n",
        "    \"bq_py_2016_minus_ethpy150\",\n",
        "    source=seqio.TextLineDataSource(\n",
        "        split_to_filepattern=codeT5.tasks.bq_py_2016_minus_ethpy150_paths,\n",
        "        num_input_examples={\"train\": 5884757, \"validation\": 1292044},\n",
        "    ),\n",
        "    preprocessors=[\n",
        "        codeT5.tasks.fl_preprocessor,\n",
        "        seqio.preprocessors.tokenize,\n",
        "        seqio.CacheDatasetPlaceholder(),\n",
        "    ],\n",
        "    metric_fns=[],\n",
        "    output_features=codeT5.tasks.DEFAULT_OUTPUT_FEATURES)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u2nKb9i6HuyH",
        "outputId": "9fbb39bf-9646-4a9f-e014-d7e62ddd4b95"
      },
      "outputs": [],
      "source": [
        "seqio.TaskRegistry.names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0zDZMglMWdFX",
        "outputId": "e3a09c08-2d72-4fc3-e770-fd39db3f48c9"
      },
      "outputs": [],
      "source": [
        "gh_task = seqio.TaskRegistry.get(\"bq_py_2016_minus_ethpy150\")\n",
        "seqio.utils.add_global_cache_dirs([os.path.join(BASE_DIR, \"cache\")])\n",
        "raw_ds = gh_task.get_dataset(sequence_length=None, split=\"validation\", use_cached=True)\n",
        "vocab = codeT5.tasks.vocab\n",
        "\n",
        "ds_len = raw_ds.map(lambda x: tf.size(x['targets']))\n",
        "\n",
        "for ex in tfds.as_numpy(ds_len.take(5)):\n",
        "  print(ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qGfC-c-a_1J"
      },
      "outputs": [],
      "source": [
        "# did not finish on TXT 1.5h on Collab, but took 20 min on cached dataset\n",
        "df = tfds.as_dataframe(ds_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iHdob6ym0K4",
        "outputId": "4d94f4ef-8e88-4832-ee1f-7f3f5e5ca5c5"
      },
      "outputs": [],
      "source": [
        "df.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "pulcW72Cmhr-",
        "outputId": "9f1bc987-efd5-4900-b383-8a381832a5c4"
      },
      "outputs": [],
      "source": [
        "df.hist(bins=30, log=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-a1YJIocBp62"
      },
      "outputs": [],
      "source": [
        "df.to_pickle(\"/content/bq_val_len.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6cepmSoquWSI"
      },
      "outputs": [],
      "source": [
        "raw_train_ds = gh_task.get_dataset(sequence_length=None, split=\"train\", use_cached=True)\n",
        "ds_train_len = raw_train_ds.map(lambda x: tf.size(x['targets']))\n",
        "df_train = tfds.as_dataframe(ds_train_len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bECTOnDDDEtF",
        "outputId": "5512cbd0-b97e-406f-c525-06915584f286"
      },
      "outputs": [],
      "source": [
        "df_train.max()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "UwHAwEoSujb-",
        "outputId": "1306256f-5503-4a89-9583-83e3f0a55576"
      },
      "outputs": [],
      "source": [
        "df_train.hist(bins=30, log=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FksdcC-M6Cpy"
      },
      "outputs": [],
      "source": [
        "df.to_pickle(\"/content/bq_train_len.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40Xc67c-6UnH",
        "outputId": "e03e86d3-83dc-40ee-e29d-1f83ffc46fa8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Abj0evJk6WHF"
      },
      "outputs": [],
      "source": [
        "!cp /content/bq_val_len.pkl '/content/drive/My Drive/'\n",
        "!cp /content/bq_train_len.pkl '/content/drive/My Drive/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rQiRJhy2FKl"
      },
      "source": [
        "# Monitoring"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrUGGGwKmHoR",
        "outputId": "ca8a1a89-024f-4017-b535-25745b126311"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "pip install -U tensorboard-plugin-profile #\"cloud-tpu-profiler>=2.3.0\"\n",
        "\n",
        "# fixes a bug in the plugin that prevents loading profile samples when the run name is \".\"\n",
        "# TODO(bzz): submit a patch upstream to 'tensorflow/profiler'\n",
        "plugin='/usr/local/lib/python3.7/dist-packages/tensorboard_plugin_profile/profile_plugin.py'\n",
        "if [[ -f \"${plugin}\" ]]; then\n",
        "  patch \"${plugin}\" < code-t5/profile_plugin.patch || echo \"Patching ${plugin} failed\"\n",
        "fi\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 821
        },
        "id": "T-OvTSlgJgxf",
        "outputId": "d4247895-b1bb-4c5f-cae1-436e123d7599"
      },
      "outputs": [],
      "source": [
        "if ON_CLOUD:\n",
        "  %reload_ext tensorboard\n",
        "%tensorboard --logdir=\"$MODEL_DIR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YQ2XYy3dWm_"
      },
      "outputs": [],
      "source": [
        "!kill 2273"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YRNVZBNW9v5"
      },
      "outputs": [],
      "source": [
        "10.76.12.218:8466"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-d6RTzGbW9b7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miNB933xYklP"
      },
      "source": [
        "# Cache\n",
        "\n",
        "Cache the dataset in .tfrecord format (depends on Apache Beam) on GCS.\n",
        "\n",
        "Needs to be done only once for each Task (dataset), all 4 Python datasets are already cached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KEkBh0_JiIx3"
      },
      "outputs": [],
      "source": [
        "!pip install apache-beam[gcp] python-snappy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yALEltupYj_F",
        "outputId": "11125e35-a927-4bb3-de9f-399b9972b796"
      },
      "outputs": [],
      "source": [
        "# works only with TextLineDataSource\n",
        "!cd code-t5 && python -m seqio.scripts.cache_tasks_main \\\n",
        " --tasks=py_50stars_top5k_2019 \\\n",
        " --module_import=codeT5.tasks \\\n",
        " --output_cache_dir='gs://t5-codex/cache' \\\n",
        " --alsologtostderr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elAMBGl9nbIq"
      },
      "source": [
        "# Train\n",
        "\n",
        "![model architecture](https://i.imgur.com/BHuHUP2.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8P7kOZVY8jNN"
      },
      "source": [
        "## bi_v1_shared prefix_lm\n",
        "\n",
        "Encoder-decoder models \\w encoder and decoder parameters shared, trained using unsupervised objective for \"prefix lm\" modeling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PflVuKGTHz1r"
      },
      "source": [
        "### Base\n",
        "\n",
        "Train 2xBERT-base 220M param model (Total size: 138M) on top5k repos with >50 stars dataset (~400M tokes)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHr-mCHQahRe",
        "outputId": "e00f6f26-7367-452a-d22f-6859ca609983"
      },
      "outputs": [],
      "source": [
        "#cache, v2-8, model_parallelism = 1\n",
        "!cd code-t5/ && python3 -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs='$BASE_DIR/cache' \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"models/shared-prefix_lm.gin\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 1\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'\" \\\n",
        "  --gin_param=\"utils.run.train_steps = $TRAIN_STEPS\" \\\n",
        "  --gin_param=\"utils.run.keep_checkpoint_max = 8\" \\\n",
        "  --gin_param=\"tokens_per_batch = 65536\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = None\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = 'fl_bq_py_mix'\" \\\n",
        "  --gin_param=\"mesh_train_dataset_fn.use_cached = True\"\n",
        "  \n",
        "  # --gin_file=\"dataset/github_python_2016.gin\" \\\n",
        "  # --gin_file=\"models/t5.1.1.base.gin\" \\\n",
        "  # --gin_file=\"objectives/prefix_lm.gin\" \\\n",
        "\n",
        "  # DOES NOT WORK ON COLAB!!! have to edit the .gin file :( \n",
        "  # --gin_param=\"utils.run.sequence_length = {'inputs': 1024, 'targets': 512}\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzvweWA0Sqd3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBxyrbFcPcSK",
        "outputId": "ab91979b-3160-4512-d44b-74ae59926eae"
      },
      "outputs": [],
      "source": [
        "!echo \"$TPU_TOPOLOGY\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-jc1_K81Iua"
      },
      "outputs": [],
      "source": [
        "# cache, v2-8, model_parallelism = 2\n",
        "!cd code-t5/ && python -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs='$BASE_DIR/cache' \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"models/shared-prefix_lm.gin\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 2\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = 2048\" \\\n",
        "  --gin_param=\"tokens_per_batch= 131072\" \\\n",
        "  --gin_param=\"run.train_steps = $TRAIN_STEPS\" \\\n",
        "  --gin_param=\"run.keep_checkpoint_max = 8\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = 'all_py_2019_mix'\" \\\n",
        "  --gin_param=\"mesh_train_dataset_fn.use_cached = True\"\n",
        "\n",
        "# utils.run batch_size = tokens_per_replica=2048, 2048*8/512*2 = 16 seq/batch, wich is overriden by tokens_per_batch = 65556\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elwEukjXis2x"
      },
      "source": [
        "10.4.42.106:8466"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gLkagXkEml9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPpOJrSHH3gj"
      },
      "source": [
        "### Large\n",
        "\n",
        "Train a larger model, 2xBERT-large 770M param (Total size: 436M), on bigger dataset (2.1B tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxNu4BEiH5AQ",
        "outputId": "07757881-f931-4557-ffb1-73b869e5ef21"
      },
      "outputs": [],
      "source": [
        "# cache, v2-8, model_parallelism = 2\n",
        "!cd code-t5/ && python -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --t5_tfds_data_dir=\"$DATA_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs='$BASE_DIR/cache' \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"models/shared-prefix_lm.gin\" \\\n",
        "  --gin_file=\"models/bi_bert_large.gin\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 2\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = 2048\" \\\n",
        "  --gin_param=\"tokens_per_batch= 131072\" \\\n",
        "  --gin_param=\"utils.run.train_steps = $TRAIN_STEPS\" \\\n",
        "  --gin_param=\"utils.run.keep_checkpoint_max = 8\" \\\n",
        "  --gin_param=\"utils.run.save_checkpoints_steps = 2000\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = 'all_py_2019_mix'\" \\\n",
        "  --gin_param=\"mesh_train_dataset_fn.use_cached = True\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brrSH4FHhyW2"
      },
      "source": [
        "## LM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vSTUMRWU6rtW"
      },
      "source": [
        "### lm_v1 lm\n",
        "\n",
        "Autoregressive single-stack Transformer (GPT-like) trained to predict next tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK0NYntR_k8u"
      },
      "outputs": [],
      "source": [
        "MODEL_SIZE=\"arch-lm_v1-lm\" \n",
        "MODEL_DIR=\"${BUCKET}/models/${MODEL_SIZE}\"\n",
        "TRAIN_STEPS=524288"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wWiEod8u-r-c"
      },
      "outputs": [],
      "source": [
        "!cd code-t5/ && python3 -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs=\"${CACHE_DIR}\" \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"dataset/dataset.gin\" \\\n",
        "  --gin_file=\"models/lm_v1.gin\" \\\n",
        "  --gin_file=\"objectives/lm.gin\" \\\n",
        "  --gin_param=\"mtf_model.MtfModel.model_type = 'lm'\" \\\n",
        "  --gin_param=\"utils.run.model_type = 'lm'\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 1\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_SIZE'\" \\\n",
        "  --gin_param=\"utils.run.train_steps = $TRAIN_STEPS\" \\\n",
        "  --gin_param=\"utils.run.sequence_length = {{'inputs': 512, 'targets': 1024}}\" \\\n",
        "  --gin_param=\"utils.run.save_checkpoints_steps = 2000\" \\\n",
        "  --gin_param=\"utils.run.keep_checkpoint_max = 25\" \\\n",
        "  --gin_param=\"tokens_per_batch = 65536\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = None\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = '${TASK_NAME}'\" \\\n",
        "  --gin_param=\"mesh_train_dataset_fn.use_cached = True\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeeOJqcJiSIz"
      },
      "source": [
        "### lm_v1_ifa prefix_lm\n",
        "\n",
        "\"delimited_lm\" a single-stack Transforme with the full attention mask over the inputs, trained on prefix LM objective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "164hueax_oID"
      },
      "outputs": [],
      "source": [
        "MODEL_SIZE = \"lm_ifa_1k\" \n",
        "MODEL_DIR = os.path.join(MODELS_DIR, MODEL_SIZE)\n",
        "TRAIN_STEPS = 524288"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZdtQppjh3LD",
        "outputId": "3d91772f-9ced-43cc-8875-21cebf75b1a3"
      },
      "outputs": [],
      "source": [
        "!cd code-t5/ && python3 -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs='$BASE_DIR/cache' \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"dataset/dataset.gin\" \\\n",
        "  --gin_file=\"models/lm_v1_ifa.gin\" \\\n",
        "  --gin_file=\"objectives/prefix_lm.gin\" \\\n",
        "  --gin_param=\"select_random_chunk.max_length = 65536\" \\\n",
        "  --gin_param=\"select_random_chunk.feature_key = 'targets'\" \\\n",
        "  --gin_param=\"mtf_model.MtfModel.model_type = 'delimited_lm'\" \\\n",
        "  --gin_param=\"utils.run.model_type = 'delimited_lm'\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 1\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'\" \\\n",
        "  --gin_param=\"utils.run.train_steps = $TRAIN_STEPS\" \\\n",
        "  --gin_param=\"utils.run.sequence_length = {{'inputs': 1024, 'targets': 512}}\" \\\n",
        "  --gin_param=\"utils.run.save_checkpoints_steps = 12000\" \\\n",
        "  --gin_param=\"utils.run.keep_checkpoint_max = 8\" \\\n",
        "  --gin_param=\"tokens_per_batch = 65536\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = None\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = 'fl_bq_py_mix'\" \\\n",
        "  --gin_param=\"mesh_train_dataset_fn.use_cached = True\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwrwsxirnW26"
      },
      "source": [
        "# Evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCrxrpeCEPwE"
      },
      "source": [
        "## With decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7kdMBgDxwX7",
        "outputId": "a73a07dd-2803-470b-a209-29322a646c88"
      },
      "outputs": [],
      "source": [
        "!cd code-t5/ && python -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs='$BASE_DIR/cache' \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"dataset/dataset.gin\" \\\n",
        "  --gin_file=\"models/lm_v1_ifa.gin\" \\\n",
        "  --gin_file=\"objectives/prefix_lm.gin\" \\\n",
        "  --gin_file=\"sample_decode.gin\" \\\n",
        "  --gin_file=\"eval.gin\" \\\n",
        "  --gin_param=\"t5.models.mesh_transformer.mesh_eval_dataset_fn.num_eval_examples = 30\" \\\n",
        "  --gin_param=\"Bitransformer.decode.temperature=0.6\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 1\" \\\n",
        "  --gin_param=\"utils.run.sequence_length = {{'inputs': 1024, 'targets': 512}}\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = None\" \\\n",
        "  --gin_param=\"split = 'validation'\" \\\n",
        "  --gin_param=\"eval_checkpoint_step = 'all'\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = 'fl_bq_py_mix'\" \\\n",
        "  --gin_param=\"Bitransformer.decode.max_decode_length = 1024\" \\\n",
        "  --gin_param=\"mesh_eval_dataset_fn.use_cached = True\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6Ab49UmEVEK"
      },
      "source": [
        "## Perplexity Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xsFVyJUJxw23",
        "outputId": "b1d496b7-1458-4632-96f3-9e3f2e0d0309"
      },
      "outputs": [],
      "source": [
        "!cd code-t5/ && python -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --additional_task_cache_dirs='$BASE_DIR/cache' \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"dataset/dataset.gin\" \\\n",
        "  --gin_file=\"models/lm_v1.gin\" \\\n",
        "  --gin_file=\"objectives/lm.gin\" \\\n",
        "  --gin_file=\"perplexity_eval.gin\" \\\n",
        "  --gin_file=\"sample_decode.gin\" \\\n",
        "  --gin_param=\"Bitransformer.decode.temperature=0.6\" \\\n",
        "  --gin_param=\"utils.run.model_type = 'lm'\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.tpu_topology = '$TPU_TOPOLOGY'\" \\\n",
        "  --gin_param=\"utils.tpu_mesh_shape.model_parallelism = 1\" \\\n",
        "  --gin_param=\"utils.run.sequence_length = {{'inputs': 512, 'targets': 1024}}\" \\\n",
        "  --gin_param=\"serialize_num_microbatches.tokens_per_microbatch_per_replica = None\" \\\n",
        "  --gin_param=\"split = 'validation'\" \\\n",
        "  --gin_param=\"eval_checkpoint_step = 'all'\" \\\n",
        "  --gin_param=\"MIXTURE_NAME = 'fl_bq_py_mix'\" \\\n",
        "  --gin_param=\"Bitransformer.decode.max_decode_length = 1024\" \\\n",
        "  --gin_param=\"mesh_eval_dataset_fn.use_cached = True\"\n",
        "\n",
        "#  --gin_file=\"models/shared-prefix_lm.gin\" \\\n",
        "#  --gin_file=\"models/t5.1.1.base.gin\" \\\n",
        "#  --gin_file=\"models/bi_bert_large.gin\" \\"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3RnH2fA9HC5n"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyNvItRKa9Zn"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4oMk-CFBa99A"
      },
      "source": [
        "# Export SavedModel\n",
        "\n",
        "You can now export the trained model from the checkpoing into the format ready for serving the predictions over HTTP with [TF Serving](https://www.tensorflow.org/tfx/guide/serving)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N4U4VKj9xjrU"
      },
      "outputs": [],
      "source": [
        "PROJECT=\"data-analytics-experiments\"\n",
        "ZONE=\"europe-west4a\"\n",
        "EXPORT_DIR=os.path.join(MODEL_DIR, \"export\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKi-R9Tea_iV",
        "outputId": "67a758d9-966a-400a-c918-cffe4a77df89"
      },
      "outputs": [],
      "source": [
        "!cd code-t5/ && python -m t5.models.mesh_transformer_main \\\n",
        "  --gcp_project=\"$PROJECT\" \\\n",
        "  --tpu_zone=\"$ZONE\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5\" \\\n",
        "  --use_model_api \\\n",
        "  --gin_param=\"mtf_model.MtfModel.model_type='lm'\" \\\n",
        "  --temperature=0.6 \\\n",
        "  --keep_top_k=-1 \\\n",
        "  --mode=\"export_predict\" \\\n",
        "  --export_dir=\"$EXPORT_DIR\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_rHpj-VyV-a",
        "outputId": "803bcbc0-bb81-4aa4-e9b6-9bcb5b7fa217"
      },
      "outputs": [],
      "source": [
        "saved_model_path = os.path.join(EXPORT_DIR, max(tf.io.gfile.listdir(EXPORT_DIR)))\n",
        "\n",
        "!saved_model_cli show --dir $saved_model_path  --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICIm0YlCEnjY"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9JVQ2l3lPK8"
      },
      "source": [
        "## Using Model API\n",
        "\n",
        "Run inference locally using latest model snapshot from the Cloud Storage.\n",
        "\n",
        "Depending on the model size and the device you are using (CPU/GPU/TPU) initialization of the model and loading the computation graph may take a while (several minutes)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eQbL1fj2mRK"
      },
      "source": [
        "### CLI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0R35f_zlIbc"
      },
      "outputs": [],
      "source": [
        "%sh\n",
        "# TPU\\GPU\\CPU with latest model checkpoint\n",
        "\n",
        "cd code-t5\n",
        "wget 'https://raw.githubusercontent.com/google-research/google-research/master/cubert/source_code.py.test'\n",
        "\n",
        "python -m t5.models.mesh_transformer_main  \\\n",
        "  --tpu=\"$TPU_ADDRESS\" \\\n",
        "  --model_dir=\"$MODEL_DIR\" \\\n",
        "  --module_import=\"codeT5.tasks\" \\\n",
        "  --gin_location_prefix=\"codeT5/gin/\" \\\n",
        "  --gin_file=\"models/shared-prefix_lm.gin\" \\\n",
        "  --gin_file=\"models/bi_bert_large.gin\" \\\n",
        "  --gin_file=\"beam_search.gin\" \\\n",
        "  --gin_param=\"use_model_api = True\" \\\n",
        "  --checkpoint_mode=\"latest\"\n",
        "  --input_file=\"source_code.py.test\"\n",
        "  --output_file=\"source_code.py.out\"\n",
        "\n",
        "  cat source_code.py.out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYQNYYeG2oE2"
      },
      "source": [
        "### Python - pyTorch API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFSOXw2N8LD1"
      },
      "outputs": [],
      "source": [
        "bubble_sort = \"\"\"from typing import List\n",
        "\n",
        "def bubble_sort(numbers: List[int]):\n",
        "    \\\"\\\"\\\" Sort given array of numbers in assending order using Bubble Sort algorithm.\n",
        "    >>> bubble_sort([3.0, 2.0, 1.0])\n",
        "    [1.0, 2.0, 3.0]\n",
        "\n",
        "    >>> bubble_sort([3.0, 1.0, 2.0])\n",
        "    [1.0, 2.0, 3.0]\n",
        "    \\\"\\\"\\\"\n",
        "\"\"\".replace(\"\\n\", \"\")\n",
        "\n",
        "has_close = \"\"\"from typing import List\n",
        "\n",
        "def has_close_elements(numbers: List[float], threshold: float) -> bool:\n",
        "    \\\"\\\"\\\" Check if in given list of numbers, are any two numbers closer to each other than\n",
        "    given threshold.\n",
        "    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\n",
        "    False\n",
        "    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\n",
        "    True\n",
        "    \\\"\\\"\\\"\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "YQ0QcPtL2Y15",
        "outputId": "4d4f5b48-ae83-4236-a6d1-0b5342ab0a9a"
      },
      "outputs": [],
      "source": [
        "# Model initialization/loading overhead for multiple consequent predicitons calls \n",
        "# can be avoided by loading the model once only though the pyTorch API.\n",
        "\n",
        "# Run it on GPU backend\n",
        "\n",
        "import functools\n",
        "import t5\n",
        "import torch\n",
        "import transformers\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "t5_config = transformers.T5Config.from_dict({\n",
        "  \"architectures\": [\n",
        "    \"T5WithLMHeadModel\"\n",
        "  ],\n",
        "  \"vocab_size\": 32000,\n",
        "  \"d_ff\": 3072,\n",
        "  \"d_kv\": 64,\n",
        "  \"d_model\": 768,\n",
        "  \"decoder_start_token_id\": 0,\n",
        "  \"dropout_rate\": 0.1,\n",
        "  \"eos_token_id\": 1,\n",
        "  \"initializer_factor\": 1.0,\n",
        "  \"is_encoder_decoder\": False,\n",
        "  \"layer_norm_epsilon\": 1e-06,\n",
        "  \"model_type\": \"t5\",\n",
        "  \"n_positions\": 512,\n",
        "  \"num_heads\": 12,\n",
        "  \"num_layers\": 12,\n",
        "  \"output_past\": True,\n",
        "  \"pad_token_id\": 0,\n",
        "  \"relative_attention_num_buckets\": 32,\n",
        "  \"task_specific_params\": {})\n",
        "\n",
        "model = t5.models.HfPyTorchModel(t5_config, \"/content/hft5/\", device)\n",
        "\n",
        "pt_model = transformers.T5EncoderModel.from_pretrained(\n",
        "    \"gs://t5-codex/models/arch-lm_v1-lm/model.ckpt-259400.index\",\n",
        "    from_tf=True,\n",
        "    config=t5_config)\n",
        "\n",
        "model._model = pt_model\n",
        "\n",
        "\n",
        "# Generate some predictions\n",
        "inputs = [\n",
        "    \"public static void \",\n",
        "    \"import ten\",\n",
        "    bubble_sort,\n",
        "    has_close,\n",
        "]\n",
        "model.predict(\n",
        "    inputs,\n",
        "    sequence_length={\"inputs\": 128},\n",
        "    batch_size=2,\n",
        ")\n",
        "\n",
        "\n",
        "# Transformers lib can load TF checkpoins into pyTorch \\wo manual conversion first (but it's slower)\n",
        "#  https://huggingface.co/transformers/main_classes/model.html#transformers.PreTrainedModel.from_pretrained\n",
        "#\n",
        "# https://github.com/google-research/text-to-text-transfer-transformer/blob/main/t5/models/mtf_model.py\n",
        "# https://github.com/google-research/text-to-text-transfer-transformer/issues/463#issuecomment-717580821\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBP0IFF_qiHy"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8Jb9FTUZtNh"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2VivgOvkgwq"
      },
      "source": [
        "## From exported SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_nmv2TyWEKF"
      },
      "outputs": [],
      "source": [
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow-text\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "somEKQ4EbULb",
        "outputId": "1d7a0ded-ff22-4009-e12f-0be40c5ad6e8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_text  # Required to run exported model.\n",
        "\n",
        "saved_model_path = os.path.join(EXPORT_DIR, max(tf.io.gfile.listdir(EXPORT_DIR)))\n",
        "\n",
        "def load_predict_fn(model_path):\n",
        "  if tf.executing_eagerly():\n",
        "    print(\"Loading SavedModel in eager mode.\")\n",
        "    imported = tf.saved_model.load(model_path, [\"serve\"])\n",
        "    return lambda x: imported.signatures['serving_default'](tf.constant(x))['outputs'].numpy()\n",
        "  else:\n",
        "    print(\"Loading SavedModel in tf 1.x graph mode.\")\n",
        "    tf.compat.v1.reset_default_graph()\n",
        "    sess = tf.compat.v1.Session()\n",
        "    meta_graph_def = tf.compat.v1.saved_model.load(sess, [\"serve\"], model_path)\n",
        "    signature_def = meta_graph_def.signature_def[\"serving_default\"]\n",
        "    print(\"Input name: \" + str(signature_def.inputs))\n",
        "    return lambda x: sess.run(\n",
        "        fetches=signature_def.outputs[\"outputs\"].name, \n",
        "        feed_dict={signature_def.inputs[\"inputs\"].name: x}\n",
        "    )\n",
        "\n",
        "predict_fn = load_predict_fn(saved_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJO-FOIhbVRP",
        "outputId": "07cd9422-a6c4-4244-9edb-f88609a4d6f6"
      },
      "outputs": [],
      "source": [
        "def answer(question):\n",
        "  return predict_fn([question])[0].decode('utf-8')\n",
        "\n",
        "for question in [\"password = \",\n",
        "                  \"def __main__():Ċ  \",\n",
        "                  \"import\",\n",
        "                  \"a\"]:\n",
        "    print(answer(question))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKfGbwBKU7I-",
        "outputId": "82b8d823-9cf6-4220-a3a4-f5c85ef0f51c"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf.debugging.set_log_device_placement(False)\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIM1m2YUXJDt",
        "outputId": "c5c5bd20-de85-48da-8b49-b7e354a566a8"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ-TZ0gJbcSv"
      },
      "source": [
        "# Docker for serving prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "He69rzZNbe2B"
      },
      "outputs": [],
      "source": [
        "export MODEL_NAME=\"py5k_prefix_lm\"\n",
        "export SAVED_MODEL_PATH=\"${PWD}/mtf-model-export\"\n",
        "\n",
        "sudo systemctl start docker\n",
        "\n",
        "gsutil cp 'gs://t5-codex/models/large/export/1630574205' $SAVED_MODEL_PATH\n",
        "\n",
        "# Download the TensorFlow Serving Docker image and repo:\n",
        "docker pull tensorflow/serving:nightly\n",
        "\n",
        "# First, run a serving image as a daemon:\n",
        "docker run -d --name serving_base tensorflow/serving:nightly\n",
        "\n",
        "# Next, copy the `SavedModel` to the container's model folder:\n",
        "docker cp $SAVED_MODEL_PATH serving_base:/models/$MODEL_NAME\n",
        "\n",
        "# Now, commit the container that's serving the model:\n",
        "docker commit --change \"ENV MODEL_NAME $MODEL_NAME\" serving_base $MODEL_NAME\n",
        "\n",
        "# Finally, save the image to a tar file:\n",
        "docker save $MODEL_NAME -o $MODEL_NAME.tar\n",
        "\n",
        "# stop `serving_base`:\n",
        "docker kill serving_base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys2yGSkgbicM"
      },
      "outputs": [],
      "source": [
        "docker run -t --rm -p 8501:8501 --name \"$MODEL_NAME-server\" $MODEL_NAME &\n",
        "\n",
        "curl -d '{\"inputs\": [\"import tensorflow \"]}' \\\n",
        "    -X POST \"http://localhost:8501/v1/models/$MODEL_NAME:predict\"\n",
        "\n",
        "docker stop \"$MODEL_NAME-server\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-4tcUT7nNMV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0iOHNojcemOC"
      },
      "outputs": [],
      "source": [
        "# 18.04 LTS https://docs.docker.com/engine/install/ubuntu/\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install \\\n",
        "    apt-transport-https \\\n",
        "    ca-certificates \\\n",
        "    curl \\\n",
        "    gnupg \\\n",
        "    lsb-release\n",
        "\n",
        "!curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
        "!echo \\\n",
        "  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu \\\n",
        "  $(lsb_release -cs) stable\" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install docker-ce docker-ce-cli containerd.io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDNbIBw8gH_r",
        "outputId": "a80fad91-d47e-44ad-e35b-f989635bed0d"
      },
      "outputs": [],
      "source": [
        "!sudo service docker stop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J-Wpo3NBfD5W",
        "outputId": "f00a0ecd-3197-4f57-dd7b-875c60a7c74d"
      },
      "outputs": [],
      "source": [
        "!sudo docker run hello-world\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yQVppTtfJl3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [
        "b4xYKKbNIG6d",
        "UABp0V_BiNm9",
        "miNB933xYklP",
        "8P7kOZVY8jNN",
        "PflVuKGTHz1r",
        "uPpOJrSHH3gj",
        "e2VivgOvkgwq",
        "YQ-TZ0gJbcSv"
      ],
      "name": "t5-code-train.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}
