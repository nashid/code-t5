#!/usr/bin/env python3
# coding=utf-8
# Copyright 2021 JetBrains.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""Converts dataset from FullLine completion format to JSONL

Input: data from https://jetbrains.team/p/ccrm/repositories/fl-dataset/files/docs/README.md
Output: JSONL schema: (content, sha, filepath, repository, license)

cat data/dataset-normalized-5000-with-imports/*.jsonl > data/jsonl/20.jsonl
rm -f data/dataset-normalized-5000-with-imports/*.jsonl
"""
import argparse
import glob
import hashlib
import json
import multiprocessing
import os
import pathlib
from collections import Counter, defaultdict
from functools import reduce
from pprint import pprint
from typing import DefaultDict

import tqdm

from code_t5.constants import NEWLINE

FILE_SIZE_LIMIT = 1 * 1024 * 1024  # 1Mb
GENERATED_HEADERS = [
    "Generated by the gRPC",
    "Generated by the protocol buffer",
    "Autogenerated by Thrift",
    "was automatically generated by SWIG",
    "giant blob of binary data",
]


class SkipException(Exception):
    def __str__(self):
        return f"files_skipped: {super().__str__()}"


def process_single_repo(lang_repo_path: str, data_dir: str, output_dir: str) -> DefaultDict[str, int]:
    """Processes a single repository
    TODO: parametarize by language
    Returns: a dict with errors count by cause
    """
    err_stats = defaultdict(int)

    branches = list(os.scandir(lang_repo_path))
    if len(branches) != 1:
        err_stats["repos_skipped_unk_branches"] = 1
        return err_stats
    branch = branches[0].name

    org_path, repo = os.path.split(lang_repo_path.rstrip(os.path.sep))
    _, org = os.path.split(org_path)

    # read metadata from 'org/repo/branch/paths.json'
    paths = os.path.join(
        data_dir, "v3", "repositories", org, repo, branch, "paths.json"  # "repositories", # top5k dataset has extra dir
    )

    try:
        with open(paths, "rt", encoding="utf-8") as m:
            metadata = json.load(m)
    except FileNotFoundError as fe:
        err_stats["repos_skipped_cannot_read_paths"] = 1
        return err_stats

    def process_file(org_repo, metadata, py_filepath, out):
        """Processes a single file in a given repo.
        Raises Skip exception if skipping this file.
        """
        filename = os.path.basename(py_filepath)
        orig_filepath = metadata[filename]
        # name, ext = os.path.splitext(filename)
        # orig_name, time = name.rsplit("_", maxsplit=1)
        # print(f"'{py_filepath}' = {orig_name}, {time}, {ext} -> {orig_filepath}")
        data = {}
        data["repository"] = org_repo
        data["size"] = os.path.getsize(py_filepath)
        data["sha"] = ""
        data["content"] = ""
        data["filepath"] = orig_filepath
        data["license"] = ""

        if data["size"] > FILE_SIZE_LIMIT:
            raise SkipException(">1mb files")  # return # skip big files

        with open(py_filepath, "rb") as f:
            bytes = f.read()
            content = bytes.decode("utf8")
            if len(content) == 0:
                raise SkipException("empty")  # return # skip empty files

            if any(s in content[:256] for s in GENERATED_HEADERS):
                raise SkipException("generated")  # return # skip generated files

            lines = content.split("\n")
            lens = [len(x) for x in lines]
            avg_len = int(reduce(lambda x, y: x + y, lens, 0) / len(lines))
            max_len = max(lens)
            if avg_len > 100 or max_len > 1000:
                raise SkipException("lines long")

            data["content"] = NEWLINE.join(lines)
            data["sha"] = hashlib.sha256(bytes).hexdigest()

        out.write(json.dumps(data, ensure_ascii=False))
        out.write("\n")

    jsonl = f"{output_dir}/{org}_{repo}.jsonl"
    # print(f"\twriting to '{jsonl}'")
    processed = 0
    with open(jsonl, "w", encoding="utf-8") as f:
        repo_py_files = pathlib.Path(f"{lang_repo_path}/{branch}/").glob("*")
        for py_filepath in repo_py_files:
            try:
                process_file(f"{org}/{repo}", metadata, py_filepath, f)
                processed += 1
            except SkipException as se:
                err_stats[str(se)] += 1
            except KeyError as ke:
                err_stats[f"path_not_found_{ke}"] += 1
            except UnicodeDecodeError as ue:
                err_stats["files_skipped: unicode"] += 1
            except Exception as e:
                err_stats[str(e)] += 1
    err_stats["repos_processed"] = 1
    err_stats["files_processed"] = processed
    return err_stats


def main(data_dir, output_dir, limit):
    all_py = os.path.join(data_dir, "v3", "languages", "Python", ".py")
    all_py_repos = f"{all_py}/*/*/"
    print(f"Listing {limit if limit else ''} repos in '{all_py_repos}'")

    os.makedirs(output_dir, exist_ok=True)

    # root, repos, _ = next(os.walk(all_py_repos))
    # repos = list(map(lambda x: os.path.join(root, x), repos))[:limit if limit else -1]
    # for _, repo in zip(range(limit), repos):
    #     print(f"{repo}")
    # for _, repo in zip(range(limit if limit else len(repos)), repos):
    #     print(f"{repo}")

    repos = glob.glob(all_py_repos)[: limit if limit else -1]
    n_repos = len(repos)

    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        input_args = zip(repos, [data_dir] * n_repos, [output_dir] * n_repos)
        results = pool.starmap(
            process_single_repo,
            tqdm.tqdm(input_args, total=len(repos)),
            chunksize=10,
        )

    # results = []
    # for repo in repos:
    #     results.append(process_single_repo(repo, args.data_dir))

    print(f"{len(results)} results")
    pprint(reduce(lambda x, y: dict(Counter(x) + Counter(y)), results, {}))


if __name__ == "__main__":
    _parser = argparse.ArgumentParser(description="Convert to FullLine dataset to JSONL")

    _parser.add_argument("--data_dir", required=True, help="path to the FullLine dataset root")
    _parser.add_argument("--output_dir", required=True, help="path to save the JSON dataset")
    _parser.add_argument("-l", "--limit", type=int, help="limit number of repos to process")

    _args = _parser.parse_args()
    main(_args.data_dir, _args.output_dir, _args.limit)
